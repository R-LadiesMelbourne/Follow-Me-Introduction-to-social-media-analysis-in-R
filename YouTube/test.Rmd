---
title: "YouTube `r icon::fa_youtube(colour='#00aced')` - tutorial UseR 2018"
author: "MariaProkofieva"
date: "08/07/2018"
output:
  html_document:
    toc: yes
    toc_depth: 3
    theme: simplex
---

Welcome to the second part of the tutorial where we are going to have a look at another popular social media platform YouTube

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages to install
```{r}
library(knitr)
#library(magick)
library(png)
library(tuber)
library(tidyverse)
library(tidytext)

library(grid)
#library(emo)
library(icon)
library(data.table)

library(psych)
library(psych)

options(stringsAsFactors = FALSE) 

```

## Reference material for the following tutorial

* [Setting up the YouTube API](https://developers.google.com/youtube/v3/getting-started)
* [Using tuber
](https://cran.r-project.org/web/packages/tuber/vignettes/tuber-ex.html)
* [Text mining with R](https://www.tidytextmining.com/index.html)

* [Topic modeling: The LDA Buffet is Now Open; or, Latent Dirichlet Allocation for English Majors](http://www.matthewjockers.net/2011/09/29/the-lda-buffet-is-now-open-or-latent-dirichlet-allocation-for-english-majors/)


## Downloading YouTube data with `tuber`

Once you setup your YouTube `r icon::fa("youtube")` access via the Google Developer Console, you can connect to the API and download data.

A reminder: to connect you use `yt_oauth()`.

```{r youtube_api, warning=FALSE, message=FALSE}
app_id = "667235664106-a5e6ng7pna0ptv7qoqnrsn0ldm1i68a8.apps.googleusercontent.com" 
app_secret = "8UV8kvt8cZ75EXg-ubOPBjDJ"

#connect
yt_oauth(app_id=app_id, app_secret=app_secret, token='', cache=FALSE)
```

By default the function  looks for `.httr-oauth` in the working directory in case you connected before. If it doesn't find it, it passes an application ID and a secret. If you do not want the function to use cache, set `cache=FALSE`.

The function launches a browser to allow you to authorize the application

Once you are connected, we can searching! 

```{r youtube_search, warning=FALSE, message=FALSE}
results <- yt_search("World Cup 2018")
kable(results[1:4,1:5])
```

The `yt_search` function returns a `data.frame` with 16 elements, including 

* `video_id`: id of the video that can be used later to get its stats 
* `publishedAt`: date of the publication
* `channelId`: id of the channel that can be used later to get access to the channel
* `title`: title of the video
* `description`: a short description of the vidl
* `channelTitle`: a more "user-friendly" name of the channel

The `yt_search` function takes parameters that let your specify your search. The most popular ones are:

* `term`: a search term to be used, searches across different types, including `video` (the default one), `channel` and `playlist`.	
* `max_results`: 	maximum number of items to be returned with the default set to 50 and maximum allowed = 500 
* `channel_id`: search videos on the channel and requires a `channel id`
* `type`: specify the type of the media and can take one three values: `video` (default), `channel`, `playlist`. 
* `published_after` and	`published_before`: specifies the timeframe for search

Now let's have a look at comments of the video. We can pick any video in our search and have a look at its comments, using video_id to get them

```{r comments, warning=FALSE, message=FALSE}
results <- get_comment_threads(c(video_id="qlZaiBuOaz4"))
kable(results[1:10, c("authorDisplayName", "textDisplay")])
```

You specify your `video id` in the `filter` argument. You can also use this argument to download comments for a specific channel. So, let's have a look at your favourite YouTube channel. Which one is your favourite? Mine is [Bloomberg](https://www.youtube.com/user/Bloomberg/featured)

But... I guess.. it's getting too busy for the day, so let's take a break and have a look at [Victoria's Secrets](https://www.youtube.com/user/VICTORIASSECRET) 

It's Brisbane, Australia after all!

```{r brisbane, echo=FALSE, fig.height=3}
img <- readPNG(file.path("YouTube tutorial-figure/brisbane.png"))
grid.raster(img)
```

To locate the channel we need to use channel id, not its name.

### Get the user/channel id

Obtaining data from YouTube `r icon::fa("youtube")` channel and have a look at its stats. is usually done through `channel_id`, which is not the same as the YouTube name you see in the YouTube link.

There are several ways to obtain your channel_id (YouTube name):

* Use YouTube Data API 
`https://www.googleapis.com/youtube/v3/channels?key={YOUR_API_KEY}&forUsername={USER_NAME}&part=id`
where `YOUR_API_KEY` is the key you create in the google developer account 
`USER_NAME` is the YouTube channel (username)

```{r channel_id, echo=FALSE, fig.height=3}
img <- readPNG(file.path("YouTube tutorial-figure/channelId1.png"))
grid.raster(img)
```

* Use the page source code
Open the YouTube page in the browser and view Source page. Search for either `externalId` or `data-channel-external-id`. The value there will be the channel id.

```{r channel_id2, echo=FALSE, fig.height=3}
img <- readPNG(file.path("YouTube tutorial-figure/channelId2.png"))
grid.raster(img)
```

###Downloading channel stats

Now that we have the channel id, lets get a list of its videos and have a look at its stats

```{r VS_search, warning=FALSE, message=FALSE}
channel_id<-"UChWXY0e-HUhoXZZ_2GlvojQ"
videosVS = yt_search(term="", type="video", channel_id = channel_id)
kable(videosVS[1:4,1:5])
#get channel stats
statsVS<-get_channel_stats(channel_id=channel_id)
statsVSSelected <- as.vector(statsVS$statistics)
results<-do.call(rbind, statsVSSelected)
head(results)
```

The `get_channel_stats` function is quite straightforward. It take `channel_id` as an argument and returns a nested list. We can select the items we need from the list and convert it to a data.frame

###Downloading stats for videos
Now that we have a list of videos from VS channel, let's download stats for each video. As an example let's do first 10

```{r VS_video_search, warning=FALSE, message=FALSE}

videosVS_sample<-videosVS[1:10,]

videoStatsVS = lapply(as.character(videosVS_sample$video_id), function(x){
  get_stats(video_id = x)
})
videoStatsVS_df = do.call(rbind.data.frame, videoStatsVS)
head(videoStatsVS_df)
```

The function uses video ids, but does not return video title and dates, which we can add ourselves and do some clean-up

```{r VS_video_search2, warning=FALSE, message=FALSE}
videoStatsVS_df$title = videosVS_sample$title
videoStatsVS_df$date = videosVS_sample$date

library(tidyverse)
videoStatsVS_df = as.tibble(videoStatsVS_df) %>%
  mutate(viewCount = as.numeric(as.character(viewCount)), #originally as factor
         likeCount = as.numeric(as.character(likeCount)),
         dislikeCount = as.numeric(as.character(dislikeCount)),
         commentCount = as.numeric(as.character(commentCount)))

head(videoStatsVS_df)
```

I ran the function with the full list of video ids for the channel and you can use this file `videoStatsVS_df.csv` under the `YouTube tutorial-data` folder. Let's load it

```{r message=FALSE}
videoStatsVS_df <- as.data.table(read.csv("YouTube tutorial-data/videoStatsVS_df.csv", stringsAsFactors=FALSE))
head(videoStatsVS_df)
```

Let's see which video was most popular:

The most view counts are:

```{r VS_popular1, warning=FALSE, message=FALSE}

videoStatsVS_df %>% arrange_(~ desc(viewCount)) %>%
  top_n(n = 5) %>% 
  select(title, viewCount, likeCount, favoriteCount, commentCount, id)
```

Let's have a look at it!

<iframe width="560" height="315" src="https://www.youtube.com/embed/vDFxWXpIgDE" frameborder="0" allowfullscreen></iframe>


The most likes are:

```{r VS_popular2, warning=FALSE, message=FALSE}

videoStatsVS_df %>% arrange_(~ desc(likeCount)) %>%
  top_n(n = 5) %>% 
  select(title, likeCount, viewCount, favoriteCount, commentCount, id)
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/wehYd9RcKDA" frameborder="0" allowfullscreen></iframe>

The most comments got:
```{r VS_popular3, warning=FALSE, message=FALSE}

videoStatsVS_df %>% arrange_(~ desc(commentCount)) %>%
  top_n(n = 5) %>% 
  select(title, commentCount, viewCount, likeCount, favoriteCount, id)
```

<iframe width="560" height="315" src="https://www.youtube.com/embed/vDFxWXpIgDE" frameborder="0" allowfullscreen></iframe>

###Downloading titles and comments
Let's have a look at titles now and see what we can find there.

Continuing with the "girl power" theme, let's compare VS to another powerhouse, US Vogue.

Following the procedure described earlier, I downloaded video stats for both VS and US Vogue channels and merge them into one file, `videostats_All.csv`

Further analysis will include manipulation with text, so we will need `tidyverse` and `tidytext` packages

```{r}
library(tidyverse)
library(tidytext)
```

you can either download the channel stats yourself (see above) or use `videostats_All.csv`

Channel ids are:
* `Americanvogue` = `UCRXiA3h1no_PFkb1JCP0yMA`
* `VICTORIASSECRET`= `UChWXY0e-HUhoXZZ_2GlvojQ`

To load the existing file let's do this

```{r message=FALSE}
videostats_All <- as.data.table(read.csv("YouTube tutorial-data/videostats_All.csv", stringsAsFactors=FALSE))
head(videostats_All)
```

Let's have a brief look at the data. We are going to use the `stargazer` package which is fantastic for generating "academic" looking results and `describeBy` function from the `psych` package that generates statistics by a grouping variable. We will group variables by channel is.

```{r message=FALSE}
library(stargazer)

stargazer(videostats_All[,.(viewCount, likeCount, commentCount, dislikeCount)], median=TRUE, digit= 1, type = "text")

library(psych)

results<-describeBy(videostats_All[, .(viewCount, likeCount, commentCount, dislikeCount)], 
          group=videostats_All$source, digits=1, mat=TRUE) 

results[,c(1:7, 10:11)]

```
Just a reminder that:
* `mean`: average
* `st. dev`: is a measure of variation in the data compared to the average
* `min` and `max`: extreme values 

Likely that the number of views relates to the number of likes: the more people view the video, the more they "like" it. We can do a correlation for this using `corr.test` function from the same `psych` package

```{r message=FALSE}
results<-corr.test(videostats_All[, .(viewCount, likeCount, commentCount, dislikeCount)], use = "complete",method="pearson",adjust="holm",
          alpha=.05,ci=FALSE)
results$r
```
Or we can have a plot it on a graph with the help of `gglot2` and `gridExtra`

```{r message=FALSE}
library (ggplot2)
library(gridExtra)
p1=ggplot(data = videostats_All[-1, ]) + geom_point(aes(x = viewCount, y = likeCount))
p2=ggplot(data = videostats_All[-1, ]) + geom_point(aes(x = viewCount, y = dislikeCount))
p3=ggplot(data = videostats_All[-1, ]) + geom_point(aes(x = viewCount, y = commentCount))
grid.arrange(p1, p2, p3, ncol = 2)
```

You just cannot LOVE Adriana Lima! But move on....

###Text analysis

As you see the `title` column has a title that describes the video. It is logically to assume that title is the first to attract attention of the viewer. Let's have a closer look and see if we can identify specific words.

Let's tokenize the title, clean it from stop words and calculate frequencies of words in the title. `Frequency` is calculated as the number of times a particular word is used in the title compared to the total number of different words used in the channel.

```{r}
title_words_All_Source<-videostats_All %>%
  as.tibble() %>% 
  unnest_tokens(word, title) %>%
  anti_join(stop_words) %>%
  count(source, word, sort = TRUE) %>%
  left_join(videostats_All %>% 
              group_by(source) %>% 
              summarise(total = n())) %>%
  mutate(freq = n/total) 
```

I saved the output into `title_words_All_Source.csv` file , so you can load it and continue working with a tokenized version of the text.

```{r message=FALSE}
title_words_All_Source <- as.data.table(read.csv("YouTube tutorial-data/title_words_All_Source.csv", stringsAsFactors=FALSE))
head(title_words_All_Source)
```







###Topic modelling

Let's see what topics we can find in the description of the videos and compare them across three channels, US Vogue, British Vogue and VS. Does the description of the video affect the number of likes, views and dislikes?

For the topic modeling we will go to the `topicmodels` package. It does not work well with `tidytext`, so we will use the `tm` package.

We will use the dataset we have already `videosVS` - it has a list of all videos from the channel as well as its description. I also downloaded the same data from  [US Vogue](https://www.youtube.com/user/Americanvogue)
and [British Vogue](https://www.youtube.com/user/vogue). I saved it to the `description_All.csv` file for you to use.

```{r message=FALSE}

library(topicmodels)
library(tm)

description_All <- as.data.table(read.csv("YouTube tutorial-data/description_All.csv", stringsAsFactors=FALSE))
description_All$source<-as.factor(description_All$source)
kable(description_All)
```
Topic modeling is a sophisticated technique that is used to classify text and identify themes. 

 The most promising and widely used algorith at this time is `Latent Dirichlet Allocation (LDA)`. The beauty of algorithm is that it allows overlapping between topics (=themes). It assumes that each document is a collection of topics. These topics can also be shared among several documents. How do we identify which topics are covered in the document?
 
We look at the document and identify specific words and in the document that relate to a particular topic. How do we do this? We just randomly assigns words to topics. 
 
Do we accurately know how many topis are there? No. We `Just Do It` and experment with different number of topics /documents/ words again and again and again to see what works best for our text. The whole processes is based on probabilities assessment, probability of a topic in a document and probability of a word in a topic. This process goes iteratively until it reaches some stable state. The document is assigned a topic(s) based on the proportion of the words assigned to each topic in this document and words are assigned to the topic based on the proportion of words assigned to the topic.


 
 recreating the documents in the corpus by adjusting the relative importance of topics in documents and words in topics iteratively.


 
 This makes it closer to natural language processing - the way we (humans) understand the text - and brings hope for accuracy. The math side of the algorithm is quite heavy. Indeed, you oare welcome to dig out the original work of [Blei et al. 2003 "Latent Dirichlet Allocation"(http://jmlr.csail.mit.edu/papers/v3/blei03a.html), but I would suggest that we try to do some practical things to get you going!
 
 

The `tm` package has a different philosophy than `tidytext`. We will be working with a `corpus` of text
source

lm(viewCount ~ sentiment )


#let's use tm package!
# Pre-process data
corpus = Corpus(VectorSource(text$txt))

corpus <- tm_map(corpus, tolower)

corpus <- tm_map(corpus, removePunctuation)

corpus <- tm_map(corpus, removeWords, stopwords("english"))

corpus <- tm_map(corpus, removeWords, UniqueLowIDF[1:500])

corpus <- tm_map(corpus, stemDocument)

dtm = DocumentTermMatrix(corpus)

# Remove sparse terms
dtm = removeSparseTerms(dtm, 0.997)

# Create data frame
labeledTerms = as.data.frame(as.matrix(dtm))

labeledTerms = labeledTerms[rowSums(abs(labeledTerms)) != 0,]

##############################################################################
#LDA Modelling Starts
###############################################################################

# set a seed so that the output of the model is predictable
news_lda <- LDA(labeledTerms, k = 4, control = list(seed = 13))

#The tidytext package provides this method for extracting the per-topic-per-word probabilities, 
# called   β  (“beta”), from the model
news_topics <- tidy(news_lda, matrix = "beta")

news_top_terms <- news_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)


news_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() + theme_bw()

#------------------------lda tuning

result <- FindTopicsNumber(
  labeledTerms,
  topics = seq(from = 2, to = 80, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result)