---
title: "Royal Tweets - useR! 2018"
author: "Anna Quaglieri & Saskia Freytag"
date: "11th Feb 2018"
output:
  github_document:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: 3
linkcolor: magenta
urlcolor: magenta
---


```{r setup, include=TRUE,cache=TRUE}
knitr::opts_chunk$set(echo = TRUE,dev = "pdf")
```

```{r}
library(twitteR)
library(tidyverse)
library(RCurl)
library(ROAuth)
library(RJSONIO)
library(devtools)
library(rtweet)

```


```{r eval = FALSE}
api_key<-'Tx6nu4td9a1L4Cy3WLYgtoxb9'
api_secret<- "VHr9jJIigl0Uvf55vf7K79uqwfiWKT5jJQJNPjstXXcj3wecne"
token <- "3108157034-Knm58WMZvymPlAAC4jhxTm2keWVHDx7XN9zCs2a"
token_secret <- "YWHNeceXflljBNjUCW99hsrY1OPV8e7euWIFVTsrmjTmP"

twitteR::setup_twitter_oauth(api_key, api_secret, token, token_secret)
```


# Dowload tweets

```{r eval=FALSE}
twittes_rladiesAU <- twitteR::searchTwitter("@RLadiesAU", n = 10000, lang= "en", since= "2016-10-01")
df <- twitteR::twListToDF(twittes_rladiesAU)


getUser_wrapper <- function(name_block){
  user_infos <- twitteR::lookupUsers(name_block, includeNA = FALSE)
  user_infosToDF <- twitteR::twListToDF(user_infos)
  return(user_infosToDF)
}

users <- getUser_wrapper(df$screenName)
```

## Text cleaning

```{r eval=FALSE}
library(tm)
## Create a world clous from @atlsexyslim tweets
# Extract tweets 
some_txt <- df$text
some_txt <- iconv(some_txt, 'UTF-8', 'latin1', 'byte')
# Clean text
# remove punctuation
some_txt <- gsub("[[:punct:]]", "", some_txt)
# remove numbers
some_txt <- gsub("[[:digit:]]", "", some_txt)
# remove html links
some_txt <- gsub("http\\w+", "", some_txt)
# remove unnecessary spaces
some_txt <- gsub("[ \t]{2,}", "", some_txt)
some_txt <- gsub("^\\s+|\\s+$", "", some_txt)
some_txt <- gsub("\n", " ", some_txt)

# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(some_txt))
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# keep a copy for stem completion later
myCorpusCopy <- myCorpus
```

# twitter timeline

```{r eval=FALSE}
tweets <- userTimeline("RLadiesAU", n = 3200)
df_tweets <- twitteR::twListToDF(tweets)
(n.tweet <- length(tweets))
writeLines(strwrap(df_tweets$text[10], 60))
```

# Dowload tweets with Rtweets package

## Tweets to search

Tutorial at \url[https://cran.r-project.org/web/packages/rtweet/vignettes/auth.html]

```{r eval=FALSE}
Hashtags <- read_csv("Hashtags.csv", col_names = FALSE)
```


```{r eval=FALSE}
## name of app you created
appname <- "SaskiaFreytag"

## api consumer
key <- 'O3G4prQHtVY8mV6XH0BbDFqwy'

## api comsumer secret 
secret <- "nGqAZNnPaqqztuGlQnVYvOoLfmXjuZssMu4JhG2SZiKnQsDhqA"

## create a token, storing it as object 'twitter_token'
twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret
)

##### Rladies AU
# Retrieve Tweets with Rladies in it
rla_tweet <- search_tweets(paste0(Hashtags$X1,collapse=" OR ") , n = 3000, token = twitter_token, lang='en')
class(rla_tweet)
colnames(rla_tweet)
summary(sapply(rla_tweet,length))


dir.create(file.path("twitter_data"),showWarnings = FALSE)
write.csv(rla_tweet, file=file.path("twitter_data",paste0("RoyalWedding_search_tweets_",Sys.Date(),".csv")),row.names = FALSE)

## access and preview data on the users who posted the tweets
us <- users_data(rla_tweet) 
write.csv(us,file.path("twitter_data",paste0("RLAU_search_tweets_userData_",Sys.Date(),".csv")),row.names = FALSE)


## return 200 tweets from @KyloR3n's timeline
rladies_au_timeline <- get_timeline("RLadiesAU", n = 2000)
head(rladies_au_timeline)
write.csv(rladies_au_timeline,file.path("twitter_data",paste0("RLAU_timeline_",Sys.Date(),".csv")),row.names = FALSE)

## extract RladiesAu's user data
user_data_rldies_au <- users_data(rladies_au_timeline)
write.csv(user_data_rldies_au,file.path("twitter_data",paste0("RLAU_timeline_userData_",Sys.Date(),".csv")),row.names = FALSE)


####################################
##### All the mentions with R-Ladies
####################################
rladies <- search_users("RLadies", n = 10000)
rladies1 <- search_users("R-Ladies", n = 10000)
combine_rladies <- rbind(rladies,rladies1)[!duplicated(rbind(rladies,rladies1)),]
grep_rlad <- combine_rladies$screen_name[grep("RLadies",combine_rladies$screen_name)]
rlad_tweeters <- lookup_users(grep_rlad)
rlad <- tweets_data(rlad_tweeters)
# for everyone get latest 
```

```{r}
rla_tweet_df <- read.csv(file.path("twitter_data",paste0("RLAU_search_tweets_","2017-11-03",".csv")))
us <- read.csv(file.path("twitter_data",paste0("RLAU_search_tweets_userData_","2017-11-03",".csv")))
rladies_au_timeline <- read.csv(file.path("twitter_data",paste0("RLAU_timeline_","2017-11-03",".csv")))
timeline_us <- read.csv(file.path("twitter_data",paste0("RLAU_timeline_userData_","2017-11-03",".csv")))

# combine tweet from timeline and tweet with R-Ladies AU
rla_tweet_df$source <- "tweetRLadiesAU"
rladies_au_timeline$source <- "timelineRLadiesAU"
colnames(rladies_au_timeline)[!(colnames(rladies_au_timeline) %in% colnames(rla_tweet_df))]
colnames(rla_tweet_df)[!(colnames(rla_tweet_df) %in% colnames(rladies_au_timeline))]
colnames(timeline_us)[!(colnames(timeline_us) %in% colnames(us))]
colnames(us)[!(colnames(us) %in% colnames(timeline_us))]

merge_tweet_time <- merge(rla_tweet_df,rladies_au_timeline,all=TRUE)
merge_tweet_time <- merge_tweet_time[!duplicated(merge_tweet_time),]
merge_tweet_time <- merge_tweet_time[!duplicated(merge_tweet_time[,colnames(merge_tweet_time) != "source"]),]
merge_users <- merge(us,timeline_us,all=TRUE)
merge_tweet_time_us <- merge(merge_tweet_time,merge_users[,c("user_id","name","screen_name","location","description","followers_count",
                                                             "friends_count","listed_count","created_at","time_zone")],all.x=TRUE)

```
